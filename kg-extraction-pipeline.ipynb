{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815a6d70-8270-4c4d-af65-fc7a3c641645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355992f-ae65-4383-8b27-f83de61cb97f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TEI Preprocessing\n",
    "We preprocess a PDF of our source material: *Graph Representation Learning* by Hamilton, available [here](https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf).\n",
    "\n",
    "Text extraction is done following Alpizar-Chacon & Sosnovsky, 2020.\n",
    "\n",
    "Their data pipeline is available as a web service at https://intextbooks.science.uu.nl/.\n",
    "\n",
    "The code for the TEI pipeline is available on Github ([link](https://github.com/intextbooks/ITCore?tab=readme-ov-file)), but requires the deployment and coordination of multiple software components. Specifically, it requires MySQL, Apache Jena, and a partial local copy of DBPedia. We use the web service to avoid the effort of deploying the extraction pipeline locally.\n",
    "\n",
    "We optionally enabled \"identify index terms in text\" and \"link entities to DBPedia\" using the category \"https://<span/>dbpedia.org/page/Category:Technology.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd79ce0-14f9-4399-bfe5-a5872d59f6de",
   "metadata": {},
   "source": [
    "### XML Data-Munging\n",
    "We process the XML output of the TEI pipeline as described in Yao 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f58e87-3d64-40df-a4cc-eedff226c3a2",
   "metadata": {},
   "source": [
    "#### install stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a87f48f-4f1d-48af-9780-f0b13d28176d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict==0.13.0 in /opt/conda/lib/python3.11/site-packages (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6349a37d-3412-4c2a-b3ac-5e142bed8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef945b-eb71-465b-8519-51caea19aec8",
   "metadata": {},
   "source": [
    "#### ingest xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ab9fbe-88d8-437c-88ae-ae0a9732be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"DLB_TEI/teiModel.xml\")\n",
    "f = open(\"DLB_TEI/teiModel.xml\")\n",
    "\n",
    "book = xmltodict.parse(\n",
    "    f.read(),\n",
    "    xml_attribs=True,\n",
    ")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f1491-9f32-4924-947b-ba36afdc4d6d",
   "metadata": {},
   "source": [
    "#### get section headings from table of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93eac41-9bbd-40f2-a836-dcb4bbcad45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl the XML tree in search of items with text\n",
    "# return a list of table of contents item headings\n",
    "def grab_toc_headings(node):\n",
    "    items = []\n",
    "    if type(node) is dict:\n",
    "        keys = node.keys()\n",
    "        if \"#text\" in keys:\n",
    "            tup = (\n",
    "                node[\"#text\"],\n",
    "                node[\"ref\"].get(\"@target\", \"NO_TARGET\"),\n",
    "                \n",
    "            )\n",
    "            items.append(tup)\n",
    "        if \"item\" in keys:\n",
    "            items += grab_toc_headings(node[\"item\"])\n",
    "        if \"list\" in keys:\n",
    "            items += grab_toc_headings(node[\"list\"])\n",
    "    if type(node) is list:\n",
    "        for elem in node:\n",
    "            items += grab_toc_headings(elem)\n",
    "    return items\n",
    "\n",
    "# remove section numbers from heading text like \"1.2.3 foo bar section\"\n",
    "def strip_toc_headings(lst):\n",
    "    return [\n",
    "        # (heading.split(\" \", maxsplit=1)[-1].strip(), ref) for heading, ref in lst\n",
    "        (re.split(r\"\\d+\", heading)[-1].strip(), ref) for heading, ref in lst\n",
    "    ]\n",
    "\n",
    "table_of_contents = book[\"TEI\"][\"front\"][\"div\"]\n",
    "toc_headings = grab_toc_headings(table_of_contents)\n",
    "clean_toc_headings = strip_toc_headings(toc_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7788adc5-809d-4ee2-8e71-72a0f1012df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1 introduction', 'seg_1'),\n",
       " ('i applied math and machine learning basics', 'NO_TARGET'),\n",
       " ('ii deep networks: modern practices', 'NO_TARGET'),\n",
       " ('1.1 who should read this book?', 'seg_3')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc_headings[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15fbb9c0-9bdd-4ecc-8451-2fc186f56d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('introduction', 'seg_1'),\n",
       " ('i applied math and machine learning basics', 'NO_TARGET'),\n",
       " ('ii deep networks: modern practices', 'NO_TARGET'),\n",
       " ('who should read this book?', 'seg_3')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_toc_headings[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9cd6e-830a-4c17-b184-63ebb55ee66d",
   "metadata": {},
   "source": [
    "#### get index entries\n",
    "For some books, TEI fails to distinguish the *bibliography* section and the *index* section. It just combines papers, citations, and index terms. We filter these out. It also tends to interpret page ranges in bib citations (\"pages 177-228\") as if they were indexes back into the book text.\n",
    "\n",
    "We use a simple heuristic of checking the string length of items that TEI identifies as index entries and set a cutoff between the point where the actual index items end and the bibliographic citations begin.\n",
    "\n",
    "This is not completely effective, because TEI also has trouble with two-column layouts that are common in book indexes. For about 15% of the items, it produces combinations like \"Point estimator, 119 Reinforcement learning.\" This results in several relative long, garbled index items.\n",
    "\n",
    "For the Deep Learning Book, we just set a heuristic of 65 chars. In this book, it separates index items from bib citations. In other cases, it might also filter out extra-long garbled index items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e5cfc6-8382-4225-b671-22d1343cced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bibliography citations that TEI mixed into the index for some reason\n",
    "# we just use excessive length as the heuristic\n",
    "# every index item for the Deep Learning Book is under 70 characters\n",
    "def remove_overlong_items(lst):\n",
    "    return [\n",
    "        elem for elem in lst\n",
    "        if type(elem) is dict and len(elem.get(\"#text\", \"\")) < 70\n",
    "    ]\n",
    "\n",
    "# filter out stuff like \"Object detection, 444 Probability distribution\"\n",
    "def filter_combined_items(lst):\n",
    "    return [\n",
    "        elem for elem in lst\n",
    "        if len(re.split(r\",\\s+\\d+\", elem.get(\"#text\", \"\"))) == 1\n",
    "    ]\n",
    "\n",
    "# index tuples are (\"foo\", set(seg_id...))\n",
    "def grab_index_tuples(lst):\n",
    "    tuples = []\n",
    "    for elem in lst:\n",
    "        elem_name = elem[\"#text\"]\n",
    "        if \"ref\" in elem.keys():\n",
    "            ref = elem[\"ref\"]\n",
    "            if type(ref) is dict:\n",
    "                target = ref.get(\"@target\", \"NO_TARGET\")\n",
    "                tup = (elem_name, set([target]))\n",
    "                tuples.append(tup)\n",
    "            if type(ref) is list:\n",
    "                targets = set(r.get(\"@target\", \"NO_TARGET\") for r in ref)\n",
    "                tup = (elem_name, targets)\n",
    "                tuples.append(tup)\n",
    "    return tuples\n",
    "\n",
    "all_pairs = lambda lst: itertools.permutations(lst, 2)\n",
    "\n",
    "# for index items that have a \"FOO, see BAR\"\n",
    "# we make a dict of all pairs \"foo=bar\" and \"bar=foo\"\n",
    "# all lowercase, for canonical lookups\n",
    "def grab_index_aliases(lst):\n",
    "    alias_dict = {}\n",
    "    sameas_uri = \"owl:sameAs\"\n",
    "    for elem in lst:\n",
    "        elem_name = elem[\"#text\"]\n",
    "        ref = elem.get(\"seg\", {}).get(\"ref\", {})\n",
    "        \n",
    "        if type(ref) is dict and ref.get(\"@property\", \"\") == sameas_uri:\n",
    "            equivalents = map(str.lower, [\n",
    "                elem_name,\n",
    "                normalize_prop_uri(ref[\"@resource\"]),\n",
    "            ])\n",
    "            alias_dict.update(all_pairs(equivalents))\n",
    "\n",
    "        if type(ref) is list:\n",
    "            equivalents = map(str.lower, [\n",
    "                elem_name,\n",
    "                *(\n",
    "                    normalize_prop_uri(r[\"@resource\"])\n",
    "                    for r in ref\n",
    "                    if r.get(\"@property\", \"\") == sameas_uri\n",
    "                ),\n",
    "            ])\n",
    "            alias_dict.update(all_pairs(equivalents))\n",
    "    \n",
    "    return alias_dict\n",
    "\n",
    "# add the \"FOO, see BAR\" terms to the index dict\n",
    "# with the same segments as BAR\n",
    "def enrich_with_aliases(index_dict, alias_dict):\n",
    "    index_dict_copy = dict(index_dict)\n",
    "    keys = list(index_dict_copy.keys())\n",
    "    keys_lower = list(map(str.lower, keys))\n",
    "    for key, alias in alias_dict.items():\n",
    "        if key not in keys_lower:\n",
    "            matching_term = next((k for k in keys if k.lower() == alias), None)\n",
    "            if matching_term != None:\n",
    "                index_dict_copy[key] = index_dict[matching_term]\n",
    "    return index_dict_copy\n",
    "\n",
    "# property URIs look something like https://intextbooks.science.uu.nl/model/XXX/property_name\n",
    "model_domain = book[\"TEI\"][\"teiHeader\"][\"fileDesc\"][\"publicationStmt\"][\"pubPlace\"]\n",
    "# model_id = book[\"TEI\"][\"@xml:id\"]\n",
    "model_id = book[\"TEI\"][\"teiHeader\"][\"fileDesc\"][\"titleStmt\"][\"title\"].rsplit(\" \", maxsplit=1)[-1]\n",
    "model_property_uri_prefix = f\"{model_domain}model/{model_id}/\"\n",
    "normalize_prop_uri = lambda s: s.removeprefix(model_property_uri_prefix).replace(\"_\", \" \").lower()\n",
    "# normalize_prop_uri = lambda s: s\n",
    "\n",
    "index_items = book[\"TEI\"][\"back\"][\"div\"][\"list\"][\"item\"]\n",
    "index_items_filtered = filter_combined_items(remove_overlong_items(index_items))\n",
    "index_tuples = grab_index_tuples(index_items_filtered)\n",
    "alias_dict = grab_index_aliases(index_items_filtered)\n",
    "index_dict = dict(index_tuples)\n",
    "index_dict_all = enrich_with_aliases(index_dict, alias_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68692762-3235-4a90-8922-9c7b10cc03ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [*itertools.islice(index_dict_all.items(), 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c113f5e8-b2df-42ec-8c1a-2805e8b01bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorted([len(elem[\"#text\"]) for elem in index_items if type(elem) is dict], reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695de4d-fb47-4505-abc8-4df8b5490320",
   "metadata": {},
   "source": [
    "#### Assembling Book Data into Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f1b146-d850-4c85-9cee-39f36ca75dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "segment2indexitem = defaultdict(set)\n",
    "for item, segs in index_dict_all.items():\n",
    "    for seg in segs:\n",
    "        segment2indexitem[seg].add(item.lower())\n",
    "\n",
    "lower_toc_headings = defaultdict(set)\n",
    "for heading, seg in clean_toc_headings:\n",
    "    if seg != \"NO_TARGET\":\n",
    "        lower_toc_headings[f\"the section about {heading.lower()}\"].add(seg)\n",
    "\n",
    "triples = []\n",
    "for heading, segs in lower_toc_headings.items():\n",
    "    for seg in segs:\n",
    "        for index_item in segment2indexitem[seg]:\n",
    "            triple = (heading.lower(), \"contains\", index_item.lower())\n",
    "            triples.append(triple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a419675-ff42-464e-8b33-d5ad88b0fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num triples from book: 542\n",
      "Sample triples:\n",
      "the section about directed generative nets, contains, ais\t\n",
      "the section about directed generative nets, contains, fully-visible bayes network\t\n",
      "the section about directed generative nets, contains, generative moment matching networks\t\n",
      "the section about directed generative nets, contains, nade\t\n",
      "the section about directed generative nets, contains, annealed importance sampling\t\n",
      "the section about directed generative nets, contains, moment matching\t\n",
      "the section about directed generative nets, contains, generative adversarial networks\t\n",
      "the section about directed generative nets, contains, lapgan\t\n",
      "the section about directed generative nets, contains, dcgan\t\n",
      "the section about directed generative nets, contains, approximate bayesian computation\n"
     ]
    }
   ],
   "source": [
    "# triples from the book look like this\n",
    "_tmp = '\\t\\n'.join(\n",
    "    map(\n",
    "        lambda tup: ', '.join(tup),\n",
    "        triples[-10:],\n",
    "    )\n",
    ")\n",
    "print(f\"Num triples from book: {len(triples)}\\nSample triples:\\n{_tmp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971901e-a143-4cb4-9bf8-bc358957fffc",
   "metadata": {},
   "source": [
    "### Wikidata enrichment\n",
    "Use the Neo4J query API to look for matching entities in Wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6748b-c25b-4d3a-a263-9cb574a4c53e",
   "metadata": {},
   "source": [
    "#### Wikidata Query Notes\n",
    "See:\n",
    "- https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial\n",
    "- https://www.mediawiki.org/wiki/Wikidata_Query_Service/User_Manual\n",
    "- https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries\n",
    "\n",
    "Notes:\n",
    "- RDF resource description framework (w3c standard)\n",
    "- OWL web ontology language\n",
    "- subject-predicate-object\n",
    "- <http://www.wikidata.org/entity/Q30> x 3 or wd:Q30  wdt:P36  wd:Q61 .\n",
    "- wdt for truthy, props have a ranking of current truthiness\n",
    "- subj and prop are uri's, value not necessarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c7c976a-f6a7-4511-b6bc-c46d3026b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import urlencode\n",
    "import json\n",
    "import time\n",
    "\n",
    "wikidata_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# limit for sparql queries\n",
    "limit_results = 30\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept\": \"application/sparql-results+json\",\n",
    "}\n",
    "\n",
    "# try to map string terms to wikidata entity URIs\n",
    "# this is a very loose mapping: we don't control for ambiguous terms,\n",
    "# or distinct things with the same name\n",
    "# we just search for the term by its wikidata label,\n",
    "# attempting to match on lowercase, CAPS, and Title Case\n",
    "def get_sparql_for_entity_term_lookup(term):\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            \n",
    "            SELECT DISTINCT ?item ?itemLabel ?itemDescription\n",
    "            \n",
    "            WHERE {{\n",
    "              VALUES ( ?termAsis ?termTitle ?termLower ?termCaps )\n",
    "              \n",
    "              {{ (\n",
    "                  \"{term}\"@en\n",
    "                  \"{term.title()}\"@en\n",
    "                  \"{term.lower()}\"@en\n",
    "                  \"{term.upper()}\"@en\n",
    "              ) }}\n",
    "              \n",
    "              {{ ?item rdfs:label ?termAsis }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termTitle }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termLower }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termCaps }} .\n",
    "              \n",
    "              SERVICE wikibase:label {{\n",
    "                bd:serviceParam wikibase:language \"en\" .\n",
    "              }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "def get_sparql_for_entity_term_count_lookup(term):\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            \n",
    "            SELECT DISTINCT (COUNT(?item) AS ?count)\n",
    "            \n",
    "            WHERE {{\n",
    "              VALUES ( ?termAsis ?termTitle ?termLower ?termCaps )\n",
    "              \n",
    "              {{ (\n",
    "                  \"{term}\"@en\n",
    "                  \"{term.title()}\"@en\n",
    "                  \"{term.lower()}\"@en\n",
    "                  \"{term.upper()}\"@en\n",
    "              ) }}\n",
    "              \n",
    "              {{ ?item rdfs:label ?termAsis }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termTitle }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termLower }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termCaps }} .\n",
    "              \n",
    "              SERVICE wikibase:label {{\n",
    "                bd:serviceParam wikibase:language \"en\" .\n",
    "              }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "# For entity E, query all (E Relation Object),\n",
    "# filtering for English-language object and relation labels.\n",
    "# E has to be a full URI, e.g. \"http://www.wikidata.org/entity/Q3757\".\n",
    "def get_sparql_for_subject_position_triples(entity):\n",
    "    term, uri, label = itemgetter(\"term\", \"uri\", \"label\")(entity)\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "\n",
    "            SELECT ?subjectLabel ?predicate ?predicateEntityLabel ?object ?objectLabel {{\n",
    "\n",
    "                # suggested here: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries#Adding_labels_for_properties\n",
    "                hint:Query hint:optimizer \"None\" .\n",
    "\n",
    "                VALUES ( ?subjectLabel ) {{ (\n",
    "                    \"{label}\"\n",
    "                ) }}\n",
    "\n",
    "                <{uri}> ?predicate ?object .\n",
    "\n",
    "                # directClaim associates predicates (WDT's) with entities (WD's), and only entities have labels\n",
    "                ?predicateEntity wikibase:directClaim ?predicate .\n",
    "\n",
    "                ?predicateEntity rdfs:label ?predicateEntityLabel .\n",
    "\n",
    "                ?object rdfs:label ?objectLabel .\n",
    "\n",
    "                FILTER (\n",
    "                    lang(?predicateEntityLabel) = \"en\" &&\n",
    "                    lang(?objectLabel) = \"en\" \n",
    "                )\n",
    "\n",
    "            }}\n",
    "            LIMIT {limit_results}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "# For entity E, query all (Subject Relation Entity),\n",
    "# filtering for English-language subject and relation labels,\n",
    "# and EXCLUDING relation P921 \"main subject,\"\n",
    "# which returns results that are too numerous and too specific.\n",
    "# E has to be a full URI, e.g. \"http://www.wikidata.org/entity/Q3757\".\n",
    "def get_sparql_for_object_position_triples(entity):\n",
    "    term, uri, label = itemgetter(\"term\", \"uri\", \"label\")(entity)\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "\n",
    "            SELECT ?subject ?subjectLabel ?predicate ?predicateEntityLabel ?objectLabel {{\n",
    "\n",
    "                # suggested here: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries#Adding_labels_for_properties\n",
    "                hint:Query hint:optimizer \"None\" .\n",
    "\n",
    "                VALUES ( ?objectLabel ) {{ (\n",
    "                    \"{label}\"\n",
    "                ) }}\n",
    "\n",
    "                ?subject ?predicate <{uri}> .\n",
    "\n",
    "                # directClaim associates predicates (WDT's) with entities (WD's), and only entities have labels\n",
    "                ?predicateEntity wikibase:directClaim ?predicate .\n",
    "\n",
    "                ?predicateEntity rdfs:label ?predicateEntityLabel .\n",
    "\n",
    "                ?subject rdfs:label ?subjectLabel .\n",
    "\n",
    "                FILTER (\n",
    "                    lang(?predicateEntityLabel) = \"en\" &&\n",
    "                    lang(?subjectLabel) = \"en\"\n",
    "                )\n",
    "\n",
    "                MINUS {{\n",
    "                    # not \"X main_subject Y\" there are too many overly specific ones  \n",
    "                    ?subject wdt:P921 <{uri}> .\n",
    "                }}\n",
    "\n",
    "            }}\n",
    "            LIMIT {limit_results}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "\n",
    "def query_wikidata(sparql):\n",
    "    body = urlencode(sparql).encode()\n",
    "\n",
    "    def make_request():\n",
    "        req = Request(url=wikidata_url, headers=headers, data=body)\n",
    "        return urlopen(req)\n",
    "\n",
    "    # respect being throttled by wikidata\n",
    "    while (response := make_request()) and response.getcode() == 429:\n",
    "        print(f\"Got 429 Too Many Requests when querying for [{query}], backing off!\")\n",
    "        delay = response.getheader(\"Retry-After\") or response.getheader(\"retry-after\")\n",
    "        delay_secs = 1.05 * float(delay)\n",
    "        print(f\"Wikidata tells us to try again after [{delay}] seconds, sleeping...\")\n",
    "        time.sleep(delay_secs)\n",
    "\n",
    "    response_json = json.load(response)\n",
    "    results = response_json[\"results\"][\"bindings\"]\n",
    "    \n",
    "    # if len(results) == 0:\n",
    "    #     print(f\"Got 0 results for query {sparql['query']}!\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_subject_position_triples_for_entity(entity):\n",
    "    sparql = get_sparql_for_subject_position_triples(entity)\n",
    "    results = query_wikidata(sparql)\n",
    "    return [\n",
    "        (\n",
    "            r[\"subjectLabel\"][\"value\"].lower(),\n",
    "            r[\"predicateEntityLabel\"][\"value\"].lower(),\n",
    "            r[\"objectLabel\"][\"value\"].lower(),\n",
    "        ) for r in results\n",
    "    ]\n",
    "\n",
    "def get_object_position_triples_for_entity(entity):\n",
    "    sparql = get_sparql_for_object_position_triples(entity)\n",
    "    results = query_wikidata(sparql)\n",
    "    return [\n",
    "        (\n",
    "            r[\"subjectLabel\"][\"value\"].lower(),\n",
    "            r[\"predicateEntityLabel\"][\"value\"].lower(),\n",
    "            r[\"objectLabel\"][\"value\"].lower(),\n",
    "        ) for r in results\n",
    "    ]\n",
    "\n",
    "def get_entities_from_term(term):\n",
    "    sparql = get_sparql_for_entity_term_lookup(term)\n",
    "    results = query_wikidata(sparql)\n",
    "    return [\n",
    "        {\n",
    "            \"term\": term,\n",
    "            \"uri\": result[\"item\"][\"value\"],\n",
    "            \"label\": result[\"itemLabel\"][\"value\"],\n",
    "            \"description\": result.get(\"itemDescription\", {}).get(\"value\", \"NO_DESCRIPTION\")\n",
    "        }\n",
    "        for result in results\n",
    "    ]\n",
    "\n",
    "def get_all_triples_for_term(term):\n",
    "    entities = get_entities_from_term(term)\n",
    "    return [\n",
    "        *[\n",
    "            triple\n",
    "                for e in entities\n",
    "                for triple in get_subject_position_triples_for_entity(e)\n",
    "        ],\n",
    "        *[\n",
    "            triple\n",
    "                for e in entities\n",
    "                for triple in get_object_position_triples_for_entity(e)\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "def count_entities_for_term(term):\n",
    "    sparql = get_sparql_for_entity_term_count_lookup(term)\n",
    "    res = query_wikidata(sparql)\n",
    "    return int(res[0][\"count\"][\"value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46a126e1-ccf1-403c-a73c-4f973673f371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_triples_for_term(\"Ancestral sampling\")\n",
    "#get_object_position_triples_for_entity(get_entities_from_term(\"autoencoder\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acc5fc17-a6d9-47c9-8c3d-481b96dc00b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lou sebert', 'place of birth', 'java'),\n",
       " ('java', 'different from', 'java'),\n",
       " ('java', 'different from', 'java')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_object_position_triples_for_entity(get_entities_from_term(\"java\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1d0d549-a28e-460c-8fba-781d7aa2a0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_entities_for_term(\"autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4116976d-6ea2-4499-a9c7-8f36f0243856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted([(k, count_entities_for_term(k)) for k in index_dict_all.keys()], key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209aa4c1-1c7c-490b-bcd1-f386748777ea",
   "metadata": {},
   "source": [
    "### Putting the Dataset Together\n",
    "We have the triples of (TOC section heading, \"contains\", index term). We now enrich this set of triples by looking up index terms in Wikidata. We add triples of (Entity, Relation, index term) and (index term, Relation, Entity) with an upper limit of 50 added per item per position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c233b553-463c-46dd-be3c-bcc74e47794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num terms: 505\n"
     ]
    }
   ],
   "source": [
    "terms = set()\n",
    "\n",
    "for subj, verb, obj in triples:\n",
    "    terms.add(subj)\n",
    "    terms.add(obj)\n",
    "\n",
    "terms = sorted(terms)\n",
    "print(f\"Num terms: {len(terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13beef01-2d6c-4c86-b60d-c832aea1e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"wiki_triples.pickle\", \"wb\") as f:\n",
    "    pickle.dump(wiki_triples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9637a120-6f25-47a7-9d03-3dbc9a33522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('information entropy', 'wikidata property example', 'normal distribution'), ('numerical differentiation', 'subclass of', 'differentiation'), ('numerical differentiation', 'subclass of', 'numerical algorithm'), ('numerical differentiation', 'maintained by wikiproject', 'wikiproject mathematics'), ('partition function', 'maintained by wikiproject', 'wikiproject mathematics'), ('partition function', 'maintained by wikiproject', 'wikiproject mathematics'), ('partition function', 'subclass of', 'dimensionless quantity'), ('partition function', 'instance of', 'integer sequence'), ('partition function', 'described by source', 'encyclopÃ¦dia britannica 11th edition'), ('partition function', 'instance of', 'integer-valued function'), ('partition function', 'maintained by wikiproject', 'wikiproject mathematics'), ('perceptron', 'instance of', 'algorithm'), ('perceptron', 'discoverer or inventor', 'frank rosenblatt'), ('perceptron', 'subclass of', 'feedforward neural network'), ('perceptron', 'described by source', 'the perceptron: a probabilistic model for information storage and organization in the brain'), ('multilayer perceptron', 'based on', 'perceptron'), ('kernel perceptron', 'subclass of', 'perceptron'), ('pooling', 'part of', 'risk management'), ('pooling', 'instance of', 'wikimedia disambiguation page'), ('mining pool', 'subclass of', 'pooling')]\n"
     ]
    }
   ],
   "source": [
    "with open(\"wiki_triples.pickle\", \"rb\") as f:\n",
    "    print(pickle.load(f)[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c520fddb-5bd5-45c7-8366-d96e4d8fb880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isfile(\"wiki_triples.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c84614e9-ff0d-4760-9c6d-99557b83e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki triples file present, loading from file.\n",
      "Total Wiki triples: 3666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('accuracy', 'instance of', 'music track with vocals'),\n",
       " ('accuracy', 'performer', 'the cure'),\n",
       " ('accuracy', 'genre', 'post-punk'),\n",
       " ('accuracy', 'different from', 'accuracy'),\n",
       " ('accuracy', 'producer', 'chris parry'),\n",
       " ('accuracy', 'recording or performance of', 'accuracy'),\n",
       " ('accuracy', 'distribution format', 'music streaming'),\n",
       " ('accuracy', 'recorded at studio or venue', 'morgan studios'),\n",
       " ('accuracy', 'contributor to the creative work or subject', 'robert smith'),\n",
       " ('accuracy',\n",
       "  'contributor to the creative work or subject',\n",
       "  'michael dempsey')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "cap_per_term = 50\n",
    "dump_file = \"wiki_triples.pickle\"\n",
    "\n",
    "def load_triples():\n",
    "    with open(dump_file, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def slowly_scrape_triples_from_wiki(terms):\n",
    "    for i, term in enumerate(terms):\n",
    "        capped = False\n",
    "        if term in (\"adam\", \"independence\"):\n",
    "            continue\n",
    "        extras = get_all_triples_for_term(term)\n",
    "        if len(extras) > cap_per_term:\n",
    "            extras = extras[:cap_per_term]\n",
    "            capped = True\n",
    "        print(f\"[{i:-3d}/{len(terms)}] Got {len(extras)} triples for term {term}. {'(CAPPED)' if capped else ''}\")\n",
    "        wiki_triples.extend(extras)\n",
    "    \n",
    "    with open(dump_file, \"wb\") as f:\n",
    "        print(f\"Saving data to {dump_file}.\")\n",
    "        pickle.dump(wiki_triples, f)\n",
    "    \n",
    "    return wiki_triples\n",
    "\n",
    "if os.path.isfile(dump_file):\n",
    "    print(\"Wiki triples file present, loading from file.\")\n",
    "    wiki_triples = load_triples()\n",
    "else:\n",
    "    print(\"Wiki triples pickle file absent, starting scrape.\")\n",
    "    wiki_triples = slowly_scrape_triples_from_wiki()\n",
    "\n",
    "print(f\"Total Wiki triples: {len(wiki_triples)}\")\n",
    "wiki_triples[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
