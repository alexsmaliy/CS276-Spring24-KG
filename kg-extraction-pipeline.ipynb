{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815a6d70-8270-4c4d-af65-fc7a3c641645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355992f-ae65-4383-8b27-f83de61cb97f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TEI Preprocessing\n",
    "We preprocess a PDF of our source material: *Graph Representation Learning* by Hamilton, available [here](https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf).\n",
    "\n",
    "Text extraction is done following Alpizar-Chacon & Sosnovsky, 2020.\n",
    "\n",
    "Their data pipeline is available as a web service at https://intextbooks.science.uu.nl/.\n",
    "\n",
    "The code for the TEI pipeline is available on Github ([link](https://github.com/intextbooks/ITCore?tab=readme-ov-file)), but requires the deployment and coordination of multiple software components. Specifically, it requires MySQL, Apache Jena, and a partial local copy of DBPedia. We use the web service to avoid the effort of deploying the extraction pipeline locally.\n",
    "\n",
    "We optionally enabled \"identify index terms in text\" and \"link entities to DBPedia\" using the category \"https://<span/>dbpedia.org/page/Category:Technology.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd79ce0-14f9-4399-bfe5-a5872d59f6de",
   "metadata": {},
   "source": [
    "### XML Data-Munging\n",
    "We process the XML output of the TEI pipeline as described in Yao 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f58e87-3d64-40df-a4cc-eedff226c3a2",
   "metadata": {},
   "source": [
    "#### install stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a87f48f-4f1d-48af-9780-f0b13d28176d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict==0.13.0\n",
      "  Using cached xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6349a37d-3412-4c2a-b3ac-5e142bed8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef945b-eb71-465b-8519-51caea19aec8",
   "metadata": {},
   "source": [
    "#### ingest xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ab9fbe-88d8-437c-88ae-ae0a9732be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"DLB_TEI/teiModel.xml\")\n",
    "\n",
    "book = xmltodict.parse(\n",
    "    f.read(),\n",
    "    xml_attribs=True,\n",
    ")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f1491-9f32-4924-947b-ba36afdc4d6d",
   "metadata": {},
   "source": [
    "#### get section headings from table of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93eac41-9bbd-40f2-a836-dcb4bbcad45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_contents = book[\"TEI\"][\"front\"][\"div\"]\n",
    "\n",
    "# crawl the XML tree in search of items with text\n",
    "# return a list of table of contents item headings\n",
    "def grab_toc_headings(node):\n",
    "    items = []\n",
    "    if type(node) is dict:\n",
    "        keys = node.keys()\n",
    "        if \"#text\" in keys:\n",
    "            tup = (\n",
    "                node[\"#text\"],\n",
    "                node[\"ref\"].get(\"@target\", \"NO_TARGET\"),\n",
    "                \n",
    "            )\n",
    "            items.append(tup)\n",
    "        if \"item\" in keys:\n",
    "            items += grab_toc_headings(node[\"item\"])\n",
    "        if \"list\" in keys:\n",
    "            items += grab_toc_headings(node[\"list\"])\n",
    "    if type(node) is list:\n",
    "        for elem in node:\n",
    "            items += grab_toc_headings(elem)\n",
    "    return items\n",
    "\n",
    "# remove section numbers from heading text like \"1.2.3 foo bar section\"\n",
    "def strip_toc_headings(lst):\n",
    "    return [\n",
    "        (heading.split(\" \", maxsplit=1)[-1].strip(), ref) for heading, ref in lst\n",
    "    ]\n",
    "\n",
    "toc_headings = grab_toc_headings(table_of_contents)\n",
    "clean_toc_headings = strip_toc_headings(toc_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7788adc5-809d-4ee2-8e71-72a0f1012df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1 introduction', 'seg_1'),\n",
       " ('i applied math and machine learning basics', 'NO_TARGET'),\n",
       " ('ii deep networks: modern practices', 'NO_TARGET'),\n",
       " ('1.1 who should read this book?', 'seg_3')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc_headings[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15fbb9c0-9bdd-4ecc-8451-2fc186f56d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('introduction', 'seg_1'),\n",
       " ('applied math and machine learning basics', 'NO_TARGET'),\n",
       " ('deep networks: modern practices', 'NO_TARGET'),\n",
       " ('who should read this book?', 'seg_3')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_toc_headings[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9cd6e-830a-4c17-b184-63ebb55ee66d",
   "metadata": {},
   "source": [
    "#### get index entries\n",
    "For some books, TEI fails to distinguish the *bibliography* section and the *index* section. It just combines papers, citations, and index terms. We filter these out. It also tends to interpret page ranges in bib citations (\"pages 177-228\") as if they were indexes back into the book text.\n",
    "\n",
    "We use a simple heuristic of checking the string length of items that TEI identifies as index entries and set a cutoff between the point where the actual index items end and the bibliographic citations begin.\n",
    "\n",
    "This is not completely effective, because TEI also has trouble with two-column layouts that are common in book indexes. For about 15% of the items, it produces combinations like \"Point estimator, 119 Reinforcement learning.\" This results in several relative long, garbled index items.\n",
    "\n",
    "For the Deep Learning Book, we just set a heuristic of 65 chars. In this book, it separates index items from bib citations. In other cases, it might also filter out extra-long garbled index items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e5cfc6-8382-4225-b671-22d1343cced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_items = book[\"TEI\"][\"back\"][\"div\"][\"list\"][\"item\"]\n",
    "\n",
    "# remove bibliography citations that TEI mixed into the index for some reason\n",
    "# we just use excessive length as the heuristic\n",
    "# every index item for the Deep Learning Book is under 70 characters\n",
    "def remove_overlong_items(lst):\n",
    "    return [\n",
    "        elem for elem in lst\n",
    "        if len(elem.get(\"#text\", \"\")) < 70\n",
    "    ]\n",
    "\n",
    "# index tuples are (\"foo\", set(seg_id...))\n",
    "def grab_index_tuples(lst):\n",
    "    tuples = []\n",
    "    for elem in lst:\n",
    "        elem_name = elem[\"#text\"]\n",
    "        if \"ref\" in elem.keys():\n",
    "            ref = elem[\"ref\"]\n",
    "            if type(ref) is dict:\n",
    "                target = ref.get(\"@target\", \"NO_TARGET\")\n",
    "                tup = (elem_name, set([target]))\n",
    "                tuples.append(tup)\n",
    "            if type(ref) is list:\n",
    "                targets = set(r.get(\"@target\", \"NO_TARGET\") for r in ref)\n",
    "                tup = (elem_name, targets)\n",
    "                tuples.append(tup)\n",
    "    return tuples\n",
    "\n",
    "# property URIs look something like https://intextbooks.science.uu.nl/model/XXX/property_name\n",
    "model_domain = book[\"TEI\"][\"teiHeader\"][\"fileDesc\"][\"publicationStmt\"][\"pubPlace\"]\n",
    "model_id = book[\"TEI\"][\"@xml:id\"]\n",
    "model_property_uri_prefix = f\"{model_domain}model/{model_id}/\"\n",
    "\n",
    "all_pairs = lambda lst: itertools.permutations(lst, 2)\n",
    "\n",
    "normalize_prop_uri = lambda s: s.removeprefix(model_property_uri_prefix).replace(\"_\", \" \").lower()\n",
    "\n",
    "# for index items that have a \"FOO, see BAR\"\n",
    "# we make a dict of all pairs \"foo=bar\" and \"bar=foo\"\n",
    "# all lowercase, for canonical lookups\n",
    "def grab_index_aliases(lst):\n",
    "    alias_dict = {}\n",
    "    sameas_uri = \"owl:sameAs\"\n",
    "    for elem in lst:\n",
    "        elem_name = elem[\"#text\"]\n",
    "        ref = elem.get(\"seg\", {}).get(\"ref\", {})\n",
    "        \n",
    "        if type(ref) is dict and ref.get(\"@property\", \"\") == sameas_uri:\n",
    "            equivalents = map(str.lower, [\n",
    "                elem_name,\n",
    "                normalize_prop_uri(ref[\"@resource\"]),\n",
    "            ])\n",
    "            alias_dict.update(all_pairs(equivalents))\n",
    "\n",
    "        if type(ref) is list:\n",
    "            equivalents = map(str.lower, [\n",
    "                elem_name,\n",
    "                *(\n",
    "                    normalize_prop_uri(r[\"@resource\"])\n",
    "                    for r in ref\n",
    "                    if r.get(\"@property\", \"\") == sameas_uri\n",
    "                ),\n",
    "            ])\n",
    "            alias_dict.update(all_pairs(equivalents))\n",
    "    \n",
    "    return alias_dict\n",
    "\n",
    "index_items_filtered = remove_overlong_items(index_items)\n",
    "\n",
    "index_tuples = grab_index_tuples(index_items_filtered)\n",
    "alias_dict = grab_index_aliases(index_items_filtered)\n",
    "index_dict = dict(index_tuples)\n",
    "\n",
    "# add the \"FOO, see BAR\" terms to the index dict\n",
    "# with the same segments as BAR\n",
    "def enrich_with_aliases(index_dict, alias_dict):\n",
    "    index_dict_copy = dict(index_dict)\n",
    "    keys = list(index_dict_copy.keys())\n",
    "    keys_lower = list(map(str.lower, keys))\n",
    "    for key, alias in alias_dict.items():\n",
    "        if key not in keys_lower:\n",
    "            matching_term = next((k for k in keys if k.lower() == alias), None)\n",
    "            if matching_term != None:\n",
    "                index_dict_copy[key] = index_dict[matching_term]\n",
    "    return index_dict_copy\n",
    "\n",
    "index_dict_all = enrich_with_aliases(index_dict, alias_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68692762-3235-4a90-8922-9c7b10cc03ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Absolute value rectification', {'seg_103'}),\n",
       " ('Accuracy', {'seg_207'}),\n",
       " ('Activation function', {'seg_99'}),\n",
       " ('Active constraint', {'seg_71'}),\n",
       " ('AdaGrad', {'seg_151'}),\n",
       " ('Adam', {'seg_153', 'seg_211'}),\n",
       " ('Adaptive linear element', {'seg_5'}),\n",
       " ('Adversarial example', {'seg_137'}),\n",
       " ('Adversarial training', {'seg_137', 'seg_141', 'seg_267'}),\n",
       " ('Affine', {'seg_77'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*itertools.islice(index_dict_all.items(), 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c113f5e8-b2df-42ec-8c1a-2805e8b01bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[210,\n",
       " 207,\n",
       " 166,\n",
       " 165,\n",
       " 163,\n",
       " 156,\n",
       " 148,\n",
       " 133,\n",
       " 122,\n",
       " 107,\n",
       " 105,\n",
       " 79,\n",
       " 61,\n",
       " 59,\n",
       " 50,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 47,\n",
       " 46,\n",
       " 45,\n",
       " 43,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 40,\n",
       " 40,\n",
       " 39,\n",
       " 38,\n",
       " 38,\n",
       " 37,\n",
       " 37,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 35,\n",
       " 33,\n",
       " 33,\n",
       " 32,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 30,\n",
       " 30,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([len(elem[\"#text\"]) for elem in index_items], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971901e-a143-4cb4-9bf8-bc358957fffc",
   "metadata": {},
   "source": [
    "#### Wikidata enrichment\n",
    "Use the Neo4J query API to look for matching entities in Wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6748b-c25b-4d3a-a263-9cb574a4c53e",
   "metadata": {},
   "source": [
    "#### Wikidata Query Notes\n",
    "See:\n",
    "- https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial\n",
    "- https://www.mediawiki.org/wiki/Wikidata_Query_Service/User_Manual\n",
    "- https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries\n",
    "\n",
    "Notes:\n",
    "- RDF resource description framework (w3c standard)\n",
    "- OWL web ontology language\n",
    "- subject-predicate-object\n",
    "- <http://www.wikidata.org/entity/Q30> x 3 or wd:Q30  wdt:P36  wd:Q61 .\n",
    "- wdt for truthy, props have a ranking of current truthiness\n",
    "- subj and prop are uri's, value not necessarily\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c7c976a-f6a7-4511-b6bc-c46d3026b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import urlencode\n",
    "import json\n",
    "import time\n",
    "\n",
    "wikidata_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept\": \"application/sparql-results+json\",\n",
    "}\n",
    "\n",
    "# try to map string terms to wikidata entity URIs\n",
    "# this is a very loose mapping: we don't control for ambiguous terms,\n",
    "# or distinct things with the same name\n",
    "# we just search for the term by its wikidata label,\n",
    "# attempting to match on lowercase, CAPS, and Title Case\n",
    "def get_sparql_for_entity_term_lookup(term):\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            \n",
    "            SELECT DISTINCT ?item ?itemLabel ?itemDescription\n",
    "            \n",
    "            WHERE {{\n",
    "              VALUES ( ?termAsis ?termTitle ?termLower ?termCaps )\n",
    "              \n",
    "              {{ (\n",
    "                  \"{term}\"@en\n",
    "                  \"{term.title()}\"@en\n",
    "                  \"{term.lower()}\"@en\n",
    "                  \"{term.upper()}\"@en\n",
    "              ) }}\n",
    "              \n",
    "              {{ ?item rdfs:label ?termAsis }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termTitle }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termLower }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termCaps }} .\n",
    "              \n",
    "              SERVICE wikibase:label {{\n",
    "                bd:serviceParam wikibase:language \"en\" .\n",
    "              }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "# lookup of triples with URI in object position\n",
    "def get_sparql_for_subject_position_triples(entity):\n",
    "    term, uri, label = itemgetter(\"term\", \"uri\", \"label\")(entity)\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "\n",
    "            SELECT ?predicateEntityLabel ?object ?objectLabel {{\n",
    "\n",
    "                # suggested here: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries#Adding_labels_for_properties\n",
    "                hint:Query hint:optimizer \"None\" .\n",
    "\n",
    "                <{uri}> ?predicate ?object .\n",
    "\n",
    "                # directClaim associates predicates (WDT's) with entities (WD's), and only entities have labels\n",
    "                ?predicateEntity wikibase:directClaim ?predicate .\n",
    "\n",
    "                ?predicateEntity rdfs:label ?predicateEntityLabel .\n",
    "\n",
    "                ?object rdfs:label ?objectLabel .\n",
    "\n",
    "                FILTER (\n",
    "                    lang(?predicateEntityLabel) = \"en\" &&\n",
    "                    lang(?objectLabel) = \"en\" \n",
    "                )\n",
    "            \n",
    "            }}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "\n",
    "def query_wikidata(sparql):\n",
    "    body = urlencode(sparql).encode()\n",
    "\n",
    "    def make_request():\n",
    "        req = Request(url=wikidata_url, headers=headers, data=body)\n",
    "        return urlopen(req)\n",
    "\n",
    "    # respect being throttled by wikidata\n",
    "    while (response := make_request()) and response.getcode() == 429:\n",
    "        print(f\"Got 429 Too Many Requests when querying for [{query}], backing off!\")\n",
    "        delay = response.getheader(\"Retry-After\") or response.getheader(\"retry-after\")\n",
    "        delay_secs = 1.05 * float(delay)\n",
    "        print(f\"Wikidata tells us to try again after [{delay}] seconds, sleeping...\")\n",
    "        time.sleep(delay_secs)\n",
    "\n",
    "    response_json = json.load(response)\n",
    "    results = response_json[\"results\"][\"bindings\"]\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(f\"Got 0 results for query {query}!\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_subject_position_triples_for_entity(entity):\n",
    "    sparql = get_sparql_for_subject_position_triples(entity)\n",
    "    results = query_wikidata(sparql)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_entities_from_term(term):\n",
    "    sparql = get_sparql_for_entity_term_lookup(term)\n",
    "    results = query_wikidata(sparql)\n",
    "    return [\n",
    "        {\n",
    "            \"term\": term,\n",
    "            \"uri\": result[\"item\"][\"value\"],\n",
    "            \"label\": result[\"itemLabel\"][\"value\"],\n",
    "            \"description\": result.get(\"itemDescription\", {}).get(\"value\", \"NO_DESCRIPTION\")\n",
    "        }\n",
    "        for result in results\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acc5fc17-a6d9-47c9-8c3d-481b96dc00b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'object': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q494764'},\n",
       "  'predicateEntityLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'located in the administrative territorial entity'},\n",
       "  'objectLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'Walworth County'}},\n",
       " {'object': {'type': 'uri',\n",
       "   'value': 'http://www.wikidata.org/entity/Q15127012'},\n",
       "  'predicateEntityLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'instance of'},\n",
       "  'objectLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'town in the United States'}},\n",
       " {'object': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q30'},\n",
       "  'predicateEntityLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'country'},\n",
       "  'objectLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'United States of America'}},\n",
       " {'object': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q251'},\n",
       "  'predicateEntityLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'different from'},\n",
       "  'objectLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Java'}},\n",
       " {'object': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q3757'},\n",
       "  'predicateEntityLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'different from'},\n",
       "  'objectLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Java'}}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subject_position_triples_for_entity(get_entities_from_term(\"java\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0d549-a28e-460c-8fba-781d7aa2a0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
