{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815a6d70-8270-4c4d-af65-fc7a3c641645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355992f-ae65-4383-8b27-f83de61cb97f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TEI Preprocessing\n",
    "We preprocess a PDF of our source material: *Graph Representation Learning* by Hamilton, available [here](https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf).\n",
    "\n",
    "Text extraction is done following Alpizar-Chacon & Sosnovsky, 2020.\n",
    "\n",
    "Their data pipeline is available as a web service at https://intextbooks.science.uu.nl/.\n",
    "\n",
    "The code for the TEI pipeline is available on Github ([link](https://github.com/intextbooks/ITCore?tab=readme-ov-file)), but requires the deployment and coordination of multiple software components. Specifically, it requires MySQL, Apache Jena, and a partial local copy of DBPedia. We use the web service to avoid the effort of deploying the extraction pipeline locally.\n",
    "\n",
    "We optionally enabled \"identify index terms in text\" and \"link entities to DBPedia\" using the category \"https://<span/>dbpedia.org/page/Category:Technology.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd79ce0-14f9-4399-bfe5-a5872d59f6de",
   "metadata": {},
   "source": [
    "### XML Data-Munging\n",
    "We process the XML output of the TEI pipeline as described in Yao 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f58e87-3d64-40df-a4cc-eedff226c3a2",
   "metadata": {},
   "source": [
    "#### install stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a87f48f-4f1d-48af-9780-f0b13d28176d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict==0.13.0 in /opt/conda/lib/python3.11/site-packages (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6349a37d-3412-4c2a-b3ac-5e142bed8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef945b-eb71-465b-8519-51caea19aec8",
   "metadata": {},
   "source": [
    "#### ingest xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ab9fbe-88d8-437c-88ae-ae0a9732be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"DLB_TEI/teiModel.xml\")\n",
    "f = open(\"DLB_TEI/teiModel.xml\")\n",
    "\n",
    "book = xmltodict.parse(\n",
    "    f.read(),\n",
    "    xml_attribs=True,\n",
    ")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f1491-9f32-4924-947b-ba36afdc4d6d",
   "metadata": {},
   "source": [
    "#### get section headings from table of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93eac41-9bbd-40f2-a836-dcb4bbcad45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl the XML tree in search of items with text\n",
    "# return a list of table of contents item headings\n",
    "def grab_toc_headings(node):\n",
    "    items = []\n",
    "    if type(node) is dict:\n",
    "        keys = node.keys()\n",
    "        if \"#text\" in keys:\n",
    "            tup = (\n",
    "                node[\"#text\"],\n",
    "                node[\"ref\"].get(\"@target\", \"NO_TARGET\"),\n",
    "                \n",
    "            )\n",
    "            items.append(tup)\n",
    "        if \"item\" in keys:\n",
    "            items += grab_toc_headings(node[\"item\"])\n",
    "        if \"list\" in keys:\n",
    "            items += grab_toc_headings(node[\"list\"])\n",
    "    if type(node) is list:\n",
    "        for elem in node:\n",
    "            items += grab_toc_headings(elem)\n",
    "    return items\n",
    "\n",
    "# remove section numbers from heading text like \"1.2.3 foo bar section\"\n",
    "def strip_toc_headings(lst):\n",
    "    return [\n",
    "        # (heading.split(\" \", maxsplit=1)[-1].strip(), ref) for heading, ref in lst\n",
    "        (re.split(r\"\\d+\", heading)[-1].strip(), ref) for heading, ref in lst\n",
    "    ]\n",
    "\n",
    "table_of_contents = book[\"TEI\"][\"front\"][\"div\"]\n",
    "toc_headings = grab_toc_headings(table_of_contents)\n",
    "clean_toc_headings = strip_toc_headings(toc_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7788adc5-809d-4ee2-8e71-72a0f1012df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1 introduction', 'seg_1'),\n",
       " ('i applied math and machine learning basics', 'NO_TARGET'),\n",
       " ('ii deep networks: modern practices', 'NO_TARGET'),\n",
       " ('1.1 who should read this book?', 'seg_3')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc_headings[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15fbb9c0-9bdd-4ecc-8451-2fc186f56d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('introduction', 'seg_1'),\n",
       " ('i applied math and machine learning basics', 'NO_TARGET'),\n",
       " ('ii deep networks: modern practices', 'NO_TARGET'),\n",
       " ('who should read this book?', 'seg_3')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_toc_headings[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9cd6e-830a-4c17-b184-63ebb55ee66d",
   "metadata": {},
   "source": [
    "#### get index entries\n",
    "For some books, TEI fails to distinguish the *bibliography* section and the *index* section. It just combines papers, citations, and index terms. We filter these out. It also tends to interpret page ranges in bib citations (\"pages 177-228\") as if they were indexes back into the book text.\n",
    "\n",
    "We use a simple heuristic of checking the string length of items that TEI identifies as index entries and set a cutoff between the point where the actual index items end and the bibliographic citations begin.\n",
    "\n",
    "This is not completely effective, because TEI also has trouble with two-column layouts that are common in book indexes. For about 15% of the items, it produces combinations like \"Point estimator, 119 Reinforcement learning.\" This results in several relative long, garbled index items.\n",
    "\n",
    "For the Deep Learning Book, we just set a heuristic of 65 chars. In this book, it separates index items from bib citations. In other cases, it might also filter out extra-long garbled index items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04e5cfc6-8382-4225-b671-22d1343cced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bibliography citations that TEI mixed into the index for some reason\n",
    "# we just use excessive length as the heuristic\n",
    "# every index item for the Deep Learning Book is under 70 characters\n",
    "def remove_overlong_items(lst):\n",
    "    return [\n",
    "        elem for elem in lst\n",
    "        if type(elem) is dict and len(elem.get(\"#text\", \"\")) < 70\n",
    "    ]\n",
    "\n",
    "# filter out stuff like \"Object detection, 444 Probability distribution\"\n",
    "def filter_combined_items(lst):\n",
    "    return [\n",
    "        elem for elem in lst\n",
    "        if len(re.split(r\",\\s+\\d+\", elem.get(\"#text\", \"\"))) == 1\n",
    "    ]\n",
    "\n",
    "# index tuples are (\"foo\", set(seg_id...))\n",
    "def grab_index_tuples(lst):\n",
    "    tuples = []\n",
    "    for elem in lst:\n",
    "        elem_name = elem[\"#text\"]\n",
    "        if \"ref\" in elem.keys():\n",
    "            ref = elem[\"ref\"]\n",
    "            if type(ref) is dict:\n",
    "                target = ref.get(\"@target\", \"NO_TARGET\")\n",
    "                tup = (elem_name, set([target]))\n",
    "                tuples.append(tup)\n",
    "            if type(ref) is list:\n",
    "                targets = set(r.get(\"@target\", \"NO_TARGET\") for r in ref)\n",
    "                tup = (elem_name, targets)\n",
    "                tuples.append(tup)\n",
    "    return tuples\n",
    "\n",
    "all_pairs = lambda lst: itertools.permutations(lst, 2)\n",
    "\n",
    "# for index items that have a \"FOO, see BAR\"\n",
    "# we make a dict of all pairs \"foo=bar\" and \"bar=foo\"\n",
    "# all lowercase, for canonical lookups\n",
    "def grab_index_aliases(lst):\n",
    "    alias_dict = {}\n",
    "    sameas_uri = \"owl:sameAs\"\n",
    "    for elem in lst:\n",
    "        elem_name = elem[\"#text\"]\n",
    "        ref = elem.get(\"seg\", {}).get(\"ref\", {})\n",
    "        \n",
    "        if type(ref) is dict and ref.get(\"@property\", \"\") == sameas_uri:\n",
    "            equivalents = map(str.lower, [\n",
    "                elem_name,\n",
    "                normalize_prop_uri(ref[\"@resource\"]),\n",
    "            ])\n",
    "            alias_dict.update(all_pairs(equivalents))\n",
    "\n",
    "        if type(ref) is list:\n",
    "            equivalents = map(str.lower, [\n",
    "                elem_name,\n",
    "                *(\n",
    "                    normalize_prop_uri(r[\"@resource\"])\n",
    "                    for r in ref\n",
    "                    if r.get(\"@property\", \"\") == sameas_uri\n",
    "                ),\n",
    "            ])\n",
    "            alias_dict.update(all_pairs(equivalents))\n",
    "    \n",
    "    return alias_dict\n",
    "\n",
    "# add the \"FOO, see BAR\" terms to the index dict\n",
    "# with the same segments as BAR\n",
    "def enrich_with_aliases(index_dict, alias_dict):\n",
    "    index_dict_copy = dict(index_dict)\n",
    "    keys = list(index_dict_copy.keys())\n",
    "    keys_lower = list(map(str.lower, keys))\n",
    "    for key, alias in alias_dict.items():\n",
    "        if key not in keys_lower:\n",
    "            matching_term = next((k for k in keys if k.lower() == alias), None)\n",
    "            if matching_term != None:\n",
    "                index_dict_copy[key] = index_dict[matching_term]\n",
    "    return index_dict_copy\n",
    "\n",
    "# property URIs look something like https://intextbooks.science.uu.nl/model/XXX/property_name\n",
    "model_domain = book[\"TEI\"][\"teiHeader\"][\"fileDesc\"][\"publicationStmt\"][\"pubPlace\"]\n",
    "# model_id = book[\"TEI\"][\"@xml:id\"]\n",
    "model_id = book[\"TEI\"][\"teiHeader\"][\"fileDesc\"][\"titleStmt\"][\"title\"].rsplit(\" \", maxsplit=1)[-1]\n",
    "model_property_uri_prefix = f\"{model_domain}model/{model_id}/\"\n",
    "normalize_prop_uri = lambda s: s.removeprefix(model_property_uri_prefix).replace(\"_\", \" \").lower()\n",
    "# normalize_prop_uri = lambda s: s\n",
    "\n",
    "index_items = book[\"TEI\"][\"back\"][\"div\"][\"list\"][\"item\"]\n",
    "index_items_filtered = filter_combined_items(remove_overlong_items(index_items))\n",
    "index_tuples = grab_index_tuples(index_items_filtered)\n",
    "alias_dict = grab_index_aliases(index_items_filtered)\n",
    "index_dict = dict(index_tuples)\n",
    "index_dict_all = enrich_with_aliases(index_dict, alias_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79878863-a420-47a2-b575-54750d8ded20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ee4e6eea5078846db7d32ae5ea2e103d'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book[\"TEI\"][\"teiHeader\"][\"fileDesc\"][\"titleStmt\"][\"title\"].rsplit(\" \", maxsplit=1)[-1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68692762-3235-4a90-8922-9c7b10cc03ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Absolute value rectification', {'seg_103'}),\n",
       " ('Accuracy', {'seg_207'}),\n",
       " ('Activation function', {'seg_99'}),\n",
       " ('Active constraint', {'seg_71'}),\n",
       " ('AdaGrad', {'seg_151'}),\n",
       " ('Adam', {'seg_153', 'seg_211'}),\n",
       " ('Adaptive linear element', {'seg_5'}),\n",
       " ('Adversarial example', {'seg_137'}),\n",
       " ('Adversarial training', {'seg_137', 'seg_141', 'seg_267'}),\n",
       " ('Affine', {'seg_77'}),\n",
       " ('Almost everywhere', {'seg_55'}),\n",
       " ('Almost sure convergence', {'seg_83'}),\n",
       " ('Ancestral sampling', {'seg_287', 'seg_301'}),\n",
       " ('Annealed importance sampling', {'seg_321', 'seg_345', 'seg_359'}),\n",
       " ('Approximate Bayesian computation', {'seg_359'}),\n",
       " ('Approximate inference', {'seg_287'}),\n",
       " ('Artificial intelligence', {'seg_1'}),\n",
       " ('Asymptotically unbiased', {'seg_83'}),\n",
       " ('Audio', {'seg_169', 'seg_227', 'seg_77'}),\n",
       " ('Autoencoder', {'seg_1', 'seg_169', 'seg_245'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*itertools.islice(index_dict_all.items(), 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c113f5e8-b2df-42ec-8c1a-2805e8b01bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[210,\n",
       " 207,\n",
       " 166,\n",
       " 165,\n",
       " 163,\n",
       " 156,\n",
       " 148,\n",
       " 133,\n",
       " 122,\n",
       " 107,\n",
       " 105,\n",
       " 79,\n",
       " 61,\n",
       " 59,\n",
       " 50,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 47,\n",
       " 46,\n",
       " 45,\n",
       " 43,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 40,\n",
       " 40,\n",
       " 39,\n",
       " 38,\n",
       " 38,\n",
       " 37,\n",
       " 37,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 35,\n",
       " 33,\n",
       " 33,\n",
       " 32,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 30,\n",
       " 30,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([len(elem[\"#text\"]) for elem in index_items if type(elem) is dict], reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695de4d-fb47-4505-abc8-4df8b5490320",
   "metadata": {},
   "source": [
    "#### Assembling Book Data into Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7f1b146-d850-4c85-9cee-39f36ca75dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the section about introduction', 'contains', 'deep blue'),\n",
       " ('the section about introduction', 'contains', 'mlp'),\n",
       " ('the section about introduction', 'contains', 'cyc'),\n",
       " ('the section about introduction', 'contains', 'deep learning'),\n",
       " ('the section about introduction', 'contains', 'hidden layer'),\n",
       " ('the section about introduction', 'contains', 'decoder'),\n",
       " ('the section about introduction', 'contains', 'logistic regression'),\n",
       " ('the section about introduction', 'contains', 'multilayer perception'),\n",
       " ('the section about introduction', 'contains', 'chess'),\n",
       " ('the section about introduction', 'contains', 'artificial intelligence')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "segment2indexitem = defaultdict(set)\n",
    "for item, segs in index_dict_all.items():\n",
    "    for seg in segs:\n",
    "        segment2indexitem[seg].add(item.lower())\n",
    "\n",
    "lower_toc_headings = defaultdict(set)\n",
    "for heading, seg in clean_toc_headings:\n",
    "    if seg != \"NO_TARGET\":\n",
    "        lower_toc_headings[f\"the section about {heading.lower()}\"].add(seg)\n",
    "\n",
    "triples = []\n",
    "for heading, segs in lower_toc_headings.items():\n",
    "    for seg in segs:\n",
    "        for index_item in segment2indexitem[seg]:\n",
    "            triple = (heading.lower(), \"contains\", index_item.lower())\n",
    "            triples.append(triple)\n",
    "\n",
    "triples[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971901e-a143-4cb4-9bf8-bc358957fffc",
   "metadata": {},
   "source": [
    "### Wikidata enrichment\n",
    "Use the Neo4J query API to look for matching entities in Wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6748b-c25b-4d3a-a263-9cb574a4c53e",
   "metadata": {},
   "source": [
    "#### Wikidata Query Notes\n",
    "See:\n",
    "- https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial\n",
    "- https://www.mediawiki.org/wiki/Wikidata_Query_Service/User_Manual\n",
    "- https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries\n",
    "\n",
    "Notes:\n",
    "- RDF resource description framework (w3c standard)\n",
    "- OWL web ontology language\n",
    "- subject-predicate-object\n",
    "- <http://www.wikidata.org/entity/Q30> x 3 or wd:Q30  wdt:P36  wd:Q61 .\n",
    "- wdt for truthy, props have a ranking of current truthiness\n",
    "- subj and prop are uri's, value not necessarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7c976a-f6a7-4511-b6bc-c46d3026b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import urlencode\n",
    "import json\n",
    "import time\n",
    "\n",
    "wikidata_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept\": \"application/sparql-results+json\",\n",
    "}\n",
    "\n",
    "# try to map string terms to wikidata entity URIs\n",
    "# this is a very loose mapping: we don't control for ambiguous terms,\n",
    "# or distinct things with the same name\n",
    "# we just search for the term by its wikidata label,\n",
    "# attempting to match on lowercase, CAPS, and Title Case\n",
    "def get_sparql_for_entity_term_lookup(term):\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            \n",
    "            SELECT DISTINCT ?item ?itemLabel ?itemDescription\n",
    "            \n",
    "            WHERE {{\n",
    "              VALUES ( ?termAsis ?termTitle ?termLower ?termCaps )\n",
    "              \n",
    "              {{ (\n",
    "                  \"{term}\"@en\n",
    "                  \"{term.title()}\"@en\n",
    "                  \"{term.lower()}\"@en\n",
    "                  \"{term.upper()}\"@en\n",
    "              ) }}\n",
    "              \n",
    "              {{ ?item rdfs:label ?termAsis }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termTitle }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termLower }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termCaps }} .\n",
    "              \n",
    "              SERVICE wikibase:label {{\n",
    "                bd:serviceParam wikibase:language \"en\" .\n",
    "              }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "def get_sparql_for_entity_term_count_lookup(term):\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            \n",
    "            SELECT DISTINCT (COUNT(?item) AS ?count)\n",
    "            \n",
    "            WHERE {{\n",
    "              VALUES ( ?termAsis ?termTitle ?termLower ?termCaps )\n",
    "              \n",
    "              {{ (\n",
    "                  \"{term}\"@en\n",
    "                  \"{term.title()}\"@en\n",
    "                  \"{term.lower()}\"@en\n",
    "                  \"{term.upper()}\"@en\n",
    "              ) }}\n",
    "              \n",
    "              {{ ?item rdfs:label ?termAsis }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termTitle }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termLower }}\n",
    "              UNION\n",
    "              {{ ?item rdfs:label ?termCaps }} .\n",
    "              \n",
    "              SERVICE wikibase:label {{\n",
    "                bd:serviceParam wikibase:language \"en\" .\n",
    "              }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "# For entity E, query all (E Relation Object),\n",
    "# filtering for English-language object and relation labels.\n",
    "# E has to be a full URI, e.g. \"http://www.wikidata.org/entity/Q3757\".\n",
    "def get_sparql_for_subject_position_triples(entity):\n",
    "    term, uri, label = itemgetter(\"term\", \"uri\", \"label\")(entity)\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "\n",
    "            SELECT ?subjectLabel ?predicate ?predicateEntityLabel ?object ?objectLabel {{\n",
    "\n",
    "                # suggested here: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries#Adding_labels_for_properties\n",
    "                hint:Query hint:optimizer \"None\" .\n",
    "\n",
    "                VALUES ( ?subjectLabel ) {{ (\n",
    "                    \"{label}\"\n",
    "                ) }}\n",
    "\n",
    "                <{uri}> ?predicate ?object .\n",
    "\n",
    "                # directClaim associates predicates (WDT's) with entities (WD's), and only entities have labels\n",
    "                ?predicateEntity wikibase:directClaim ?predicate .\n",
    "\n",
    "                ?predicateEntity rdfs:label ?predicateEntityLabel .\n",
    "\n",
    "                ?object rdfs:label ?objectLabel .\n",
    "\n",
    "                FILTER (\n",
    "                    lang(?predicateEntityLabel) = \"en\" &&\n",
    "                    lang(?objectLabel) = \"en\" \n",
    "                )\n",
    "            \n",
    "            }}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "# For entity E, query all (Subject Relation Entity),\n",
    "# filtering for English-language subject and relation labels,\n",
    "# and EXCLUDING relation P921 \"main subject,\"\n",
    "# which returns results that are too numerous and too specific.\n",
    "# E has to be a full URI, e.g. \"http://www.wikidata.org/entity/Q3757\".\n",
    "def get_sparql_for_object_position_triples(entity):\n",
    "    term, uri, label = itemgetter(\"term\", \"uri\", \"label\")(entity)\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "            PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "\n",
    "            SELECT ?subject ?subjectLabel ?predicate ?predicateEntityLabel ?objectLabel {{\n",
    "\n",
    "                # suggested here: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries#Adding_labels_for_properties\n",
    "                hint:Query hint:optimizer \"None\" .\n",
    "\n",
    "                VALUES ( ?objectLabel ) {{ (\n",
    "                    \"{label}\"\n",
    "                ) }}\n",
    "\n",
    "                ?subject ?predicate <{uri}> .\n",
    "\n",
    "                # directClaim associates predicates (WDT's) with entities (WD's), and only entities have labels\n",
    "                ?predicateEntity wikibase:directClaim ?predicate .\n",
    "\n",
    "                ?predicateEntity rdfs:label ?predicateEntityLabel .\n",
    "\n",
    "                ?subject rdfs:label ?subjectLabel .\n",
    "\n",
    "                FILTER (\n",
    "                    lang(?predicateEntityLabel) = \"en\" &&\n",
    "                    lang(?subjectLabel) = \"en\"\n",
    "                )\n",
    "\n",
    "                MINUS {{\n",
    "                    # not \"X main_subject Y\" there are too many overly specific ones  \n",
    "                    ?subject wdt:P921 <{uri}> .\n",
    "                }}\n",
    "            \n",
    "            }}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "\n",
    "def query_wikidata(sparql):\n",
    "    body = urlencode(sparql).encode()\n",
    "\n",
    "    def make_request():\n",
    "        req = Request(url=wikidata_url, headers=headers, data=body)\n",
    "        return urlopen(req)\n",
    "\n",
    "    # respect being throttled by wikidata\n",
    "    while (response := make_request()) and response.getcode() == 429:\n",
    "        print(f\"Got 429 Too Many Requests when querying for [{query}], backing off!\")\n",
    "        delay = response.getheader(\"Retry-After\") or response.getheader(\"retry-after\")\n",
    "        delay_secs = 1.05 * float(delay)\n",
    "        print(f\"Wikidata tells us to try again after [{delay}] seconds, sleeping...\")\n",
    "        time.sleep(delay_secs)\n",
    "\n",
    "    response_json = json.load(response)\n",
    "    results = response_json[\"results\"][\"bindings\"]\n",
    "    \n",
    "    # if len(results) == 0:\n",
    "    #     print(f\"Got 0 results for query {sparql['query']}!\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_subject_position_triples_for_entity(entity):\n",
    "    sparql = get_sparql_for_subject_position_triples(entity)\n",
    "    results = query_wikidata(sparql)\n",
    "    return [\n",
    "        (\n",
    "            r[\"subjectLabel\"][\"value\"],\n",
    "            r[\"predicateEntityLabel\"][\"value\"],\n",
    "            r[\"objectLabel\"][\"value\"],\n",
    "        ) for r in results\n",
    "    ]\n",
    "\n",
    "def get_object_position_triples_for_entity(entity):\n",
    "    sparql = get_sparql_for_object_position_triples(entity)\n",
    "    results = query_wikidata(sparql)\n",
    "    return [\n",
    "        (\n",
    "            r[\"subjectLabel\"][\"value\"],\n",
    "            r[\"predicateEntityLabel\"][\"value\"],\n",
    "            r[\"objectLabel\"][\"value\"],\n",
    "        ) for r in results\n",
    "    ]\n",
    "\n",
    "def get_entities_from_term(term):\n",
    "    sparql = get_sparql_for_entity_term_lookup(term)\n",
    "    results = query_wikidata(sparql)\n",
    "    return [\n",
    "        {\n",
    "            \"term\": term,\n",
    "            \"uri\": result[\"item\"][\"value\"],\n",
    "            \"label\": result[\"itemLabel\"][\"value\"],\n",
    "            \"description\": result.get(\"itemDescription\", {}).get(\"value\", \"NO_DESCRIPTION\")\n",
    "        }\n",
    "        for result in results\n",
    "    ]\n",
    "\n",
    "def get_all_triples_for_term(term):\n",
    "    entities = get_entities_from_term(term)\n",
    "    return [\n",
    "        *[\n",
    "            triple\n",
    "                for e in entities\n",
    "                for triple in get_subject_position_triples_for_entity(e)\n",
    "        ],\n",
    "        *[\n",
    "            triple\n",
    "                for e in entities\n",
    "                for triple in get_object_position_triples_for_entity(e)\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "def count_entities_for_term(term):\n",
    "    sparql = get_sparql_for_entity_term_count_lookup(term)\n",
    "    res = query_wikidata(sparql)\n",
    "    return int(res[0][\"count\"][\"value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46a126e1-ccf1-403c-a73c-4f973673f371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_triples_for_term(\"Ancestral sampling\")\n",
    "#get_object_position_triples_for_entity(get_entities_from_term(\"autoencoder\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acc5fc17-a6d9-47c9-8c3d-481b96dc00b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('autoencoder', 'instance of', 'lossy compression'),\n",
       " ('autoencoder', 'has use', 'data compression'),\n",
       " ('autoencoder', 'subclass of', 'artificial neural network'),\n",
       " ('autoencoder', 'different from', 'Autocoder')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subject_position_triples_for_entity(get_entities_from_term(\"autoencoder\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d0d549-a28e-460c-8fba-781d7aa2a0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_entities_for_term(\"autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116976d-6ea2-4499-a9c7-8f36f0243856",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(k, count_entities_for_term(k)) for k in index_dict_all.keys()], key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209aa4c1-1c7c-490b-bcd1-f386748777ea",
   "metadata": {},
   "source": [
    "### Putting the Dataset Together\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
